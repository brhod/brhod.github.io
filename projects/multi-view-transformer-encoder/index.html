<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>AGWE with Transformer encoders | </title> <meta name="author" content="Brandon Rhodes"> <meta name="description" content="Investigating the performance of Transformer encoders when learning acoustically grounded word embeddings (AGWE) from an acoustic signal and a character string for that signal."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://brhod.github.io/projects/multi-view-transformer-encoder/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?3dd82e91913a2c1265c0f80e41ff39e2"></script> <script src="/assets/js/dark_mode.js?6458e63976eae16c0cbe86b97023895a"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/condensed_resume/">Resume (condensed)</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">AGWE with Transformer encoders</h1> <p class="post-description">Investigating the performance of Transformer encoders when learning acoustically grounded word embeddings (AGWE) from an acoustic signal and a character string for that signal.</p> </header> <article> <h2 id="overview">Overview</h2> <p>This is a project my partner <a href="https://stat.uchicago.edu/people/profile/rujual-singh-bains/" target="\_blank" rel="external nofollow noopener">Rujual Bains</a> and I did for our Speech Technologies class with Professor <a href="https://ttic.uchicago.edu/~klivescu/" target="\_blank" rel="external nofollow noopener">Karen Livescu</a> (TTIC 31190). We tried to improve upon the quality of acoustically grounded word embeddings (agwe) obtained from a multi-view setup (acoustic and character) by using Transformer encoders instead of bidirectional LSTMs for each view. These three images can summarize the concept of the project. I will go into more detail about each below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/200531_multiple_embeddings.png" alt="" title="example image"> </div> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/200531_multiview.png" alt="" title="example image"> </div> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/200531_multiview_transformer.png" alt="" title="example image"> </div> </div> <div class="caption"> Left-most image: idea of acoustic word embeddings. Middle image: the framework which we modified. Right-most image: our changes to the frame work (replacing BiLSTMs with Transformer encoders.) </div> <h2 id="acoustic-word-embeddings">Acoustic word embeddings</h2> <p>The goal of this project was to try to improve upon acoustically grounded word embeddings, but it is instructive to first know what are <em>acoustic word embeddings</em>. Acoustic word embeddings are a finite-dimensional representation of an acoustic signal of arbitrary length; in other wrods, they are a mapping of a sequence of acoustic frames to a vector. This idea can be seen in the photo below. Acoustic word embeddings are a tool gaining traction in automatic speech recognition (ASR), which is the task of recognizing a sequence of words in a language given the acoustic signal (the audio / speech itself), because of the potential increase in simplicity and efficiency they offer when compared to doing ASR with sub-word representations. They also offer interesting prospects for the representation of distance between acoustic signals, which is interesting for spoken term detection tasks.</p> <div class="row justify-content-md-center"> <img class="img-fluid rounded z-depth-1" src="/assets/img/200531_multiple_embeddings.png" alt="" title="example image"> </div> <div class="caption"> An example of two acoustic signals (i.e. sequence of frames; i.e. samples of speech) being mapped to a point (vector) in a three-dimensional space. Typically, we use spaces of dimension much greater than three, but the idea is the same. </div> <h2 id="acoustically-grounded-word-embeddings-agwe">Acoustically grounded word embeddings (agwe)</h2> <h3 id="acoustically-grounded-word-embeddings-versus-acoustic-word-embeddings">Acoustically grounded word embeddings versus acoustic word embeddings</h3> <p><em>Acoustically grounded word embeddings</em> are embeddings of written words which are based on the acoustic/phonetic content. So, the difference between acoustic word embeddings and acoustically grounded word embeddings is this: acoustic word embeddings are obtained by a function which maps a spoken word to a point in a vector space; acoustically grounded word embeddings are obtained by a function which takes a written word with some acoustic information about that word and maps it to a point in a vector space. The difference is admittedly a bit subtle, but it is important. Acoustic word embeddings are nice but limited: one of the first things you learn in a phonology course is that although we perceive each instance of the <em>basketball</em> as more or less the same, the production of this word, even within a single speaker, is incredibly variable, and this variability limits acoustic word embeddings in ASR. Acoustically grounded word embeddings, on the other hand, deal with written language, which is for the most part static, and this stability allows for more flexibility in its application, however paradoxical that may sound.</p> <h3 id="obtaining-acoustically-grounded-word-embeddings">Obtaining acoustically grounded word embeddings</h3> <p>There are several ways you can imbue word embeddings with acoustic information. The strategy <a href="https://stat.uchicago.edu/people/profile/rujual-singh-bains/" target="\_blank" rel="external nofollow noopener">Rujual</a> and I used was based on one that our professor <a href="https://ttic.uchicago.edu/~klivescu/" target="\_blank" rel="external nofollow noopener">Karen Livescu</a> and a few of her students, mainly <a href="https://github.com/shane-settle" target="\_blank" rel="external nofollow noopener">Shane Settle</a> and <a href="https://www.linkedin.com/in/wanjia-he-435004a5" target="\_blank" rel="external nofollow noopener">Wanjia He</a>, worked on. A couple of great papers for more background are by <a href="https://arxiv.org/pdf/1903.12306.pdf" target="\_blank" rel="external nofollow noopener">Settle et al.</a> and <a href="https://arxiv.org/pdf/1611.04496.pdf" target="\_blank" rel="external nofollow noopener">He et al.</a>. We had a multi-view setup like the work just referenced: we used information from the sequence of characters in the word itself (as spelling isn’t entirely arbitrary when it comes to acoustics) and information from a spoken example of that word (this signal obviously full of acoustic information). The goal was to learn a set of vectors where written words which sounded the same were closer to each other. Distance was measured by cosine similarity (which in our case is the angle between the two vectors). A more detailed version of the second image above, which shows the multi-view framework which we modified, can be found below.</p> <div class="row justify-content-md-center"> <img class="img-fluid rounded z-depth-1" src="/assets/img/200531_multiview_setup.png" alt="" title="example image"> </div> <div class="caption"> The multi-view acoustic word embedding framework which we modified. </div> <h2 id="our-modifications">Our modifications</h2> <p>Pretraining (semantic + syntactic) word embeddings with Transformers has gained a lot of popularity in Natural Language Processing (NLP), largely due to the Transformer’s efficiency in training and equal-to-better than state-of-the-art performance on many tasks. The learning of acoustically grounded word embeddings is also a pretraining task with a similar desired outcome: we want embeddings which capture meaningful linguistic information. The success of Transformers in NLP had us wonder whether this same success could be seen in speech technologies, like improving acoustically grounded word embeddings. So, <a href="https://stat.uchicago.edu/people/profile/rujual-singh-bains/" target="\_blank" rel="external nofollow noopener">Rujual</a> and I modified the multi-view framework above by substituting the bidirectional LSTMs found in the previous work with Transformer encoders to serve as the embedding (encoding) function for the acoustic signal and sequence of characters. Below, you’ll see a simplified representation of what we did as well as a more detailed representation of the component of interest, which is the Transformer encoder.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/200531_multiview_transformer.png" alt="" title="example image"> </div> <div class="col-sm-4 mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/transformer5.JPG" alt="" title="example image"> </div> </div> <div class="caption"> Left image: the modifications we made to the existing framework. Right image: the Transformer encoder itself. </div> <h2 id="results">Results</h2> <p>We unfortunately didn’t improve upon the previous approach (based on average precision), but we did do better than some other approaches. The acoustic view results pertain to just the acoustic word embeddings learned in the multi-view setup, and the cross-view results reference the joint embeddings. Few people have considered the multi-view approach, so that is why there are fewer points of comparison in the second table. Our approach did much better learning acoustic word embeddings than it did acoustically grounded word embeddings.</p> <div class="row justify-content-lg-center"> <img class="img-fluid rounded z-depth-1" src="/assets/img/210107_multiview-transformer-encoder-results.png" alt="" title="example image"> </div> <div class="caption"> Results from our project. </div> <h2 id="for-more-information">For more information</h2> <p>Please feel free to reach out to me if you have any questions regarding the project! You can check out the code at my <a href="https://github.com/rhodb/agwe_transformer" target="\_blank" rel="external nofollow noopener">repository</a>, and you can check out the paper <a href="/assets/pdf/200612_finalreport_multiview_transformer_encoder.pdf">here</a>.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Brandon Rhodes. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>