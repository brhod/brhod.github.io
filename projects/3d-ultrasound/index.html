<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>4D ultrasound imaging of /l/ and /r/ in American English | </title> <meta name="author" content="Brandon Rhodes"> <meta name="description" content="A study I helped conduct in Steven Lulich's lab of Speech and Hearing Sciences at Indiana University. The work was joint with Kelly Berkson and Ken de Jong in the linguistics department. We looked at production of /l/ and /r/ in American English with time aligned 3D ultrasound."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://brhod.github.io/projects/3d-ultrasound/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?3dd82e91913a2c1265c0f80e41ff39e2"></script> <script src="/assets/js/dark_mode.js?6458e63976eae16c0cbe86b97023895a"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/condensed_resume/">Resume (condensed)</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">4D ultrasound imaging of /l/ and /r/ in American English</h1> <p class="post-description">A study I helped conduct in Steven Lulich's lab of Speech and Hearing Sciences at Indiana University. The work was joint with Kelly Berkson and Ken de Jong in the linguistics department. We looked at production of /l/ and /r/ in American English with time aligned 3D ultrasound.</p> </header> <article> <h2 id="overview">Overview</h2> <p>This is a small page on a big project I worked on for <a href="https://sphs.indiana.edu/about/faculty/lulich-steven.html" rel="external nofollow noopener" target="_blank">Steven Lulich</a>, <a href="https://linguistics.indiana.edu/about/faculty/berkson-kelly.html" rel="external nofollow noopener" target="_blank">Kelly Berkson</a> and <a href="https://cl.indiana.edu/~kdejong/" rel="external nofollow noopener" target="_blank">Ken de Jong</a>, all at Indiana University. I only worked on it for about a year, the year before I took off for graduate school at the University of Chicago. The goal of the project was to investigate /l/ and /r/ articulation in American English, as the variants of these sounds in American English are less common typologically-speaking. To do this, we used three-dimensional ultrasound over time, making the measurements four-dimensional; so, the ultrasound transducer recorded images in three directions – forward-backward, left-right and top-bottom — and we aligned these images over the time of the production. The upshot of this was that after some processing, we could have a reconstruction of a person’s vocal tract as they produced these sounds, giving us an idea of the variance and differences in the production. The precedent / proof-of-concept set by this project was incredible. If you have the appropriate access, you can find out more <a href="https://sphs.indiana.edu/research/publications/2019-acquiring-and-visualing.html" rel="external nofollow noopener" target="_blank">here</a>. (I think in a parallel universe, I stayed at Indiana University and continued working on this project…)</p> <h2 id="pipeline">Pipeline</h2> <h3 id="getting-measurements">Getting measurements</h3> <p>I was heavily involved in the running of the experiment. There were many parts: first, we had to make an impression of someone’s palate (giving us an idea of the space the tongue is operating when we do the ultrasound imaging)… so think dentist / orthodontist, however unpleasant that may be; second, we took this palate impression and digitized it using a three-dimensional scanner. After this, we put a Frankenstein-like contraption on the person’s head, securing the ultrasound transducer underneath their chin so that we could get measurements; last, we had them go into a sound proofed room and do many recordings, where we were looking at the production of /l/’s and /r/’s in word-initial and word-final position. We used the frame <em>I said a [word-of-interest] again</em>, where the <em>word-of-interest</em> would be something like <em>roll</em> or <em>lore</em>. You can see the contraption below in the left image and then where the ultrasound transducer would go in the second image.</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/210111_palate.png" alt="" title="example image"> </div> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/210111_ultrasound-head-contraption.jpg" alt="" title="example image"> </div> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/4d2.png" alt="" title="example image"> </div> </div> <div class="caption"> Left image: example of a palate impression --- just like at the dentist or orthodontist! Middle image: the contraption which held the transducer in place (this is from a Berkeley lab, but ours was the exact same). Right image: where the ultrasound transducer would go and a visualization of the tongue that it would produce. </div> <h3 id="processing-the-data">Processing the data</h3> <p>Getting the data was tough logistically-speaking, but everything flowed harmoniously once you got the hang of it; processing the data was easy conceptually-speaking, but practically very tough to do because there was <strong><em>so</em></strong> much of it. So, another large labor of love for this study that I was deeply involved in was the processing of the data. Unfortunately, at the time, there were not sufficient machine learning methods to automate the processing, so we had to do most of it by hand. Processing the data consisted of taking each image the ultrasound transducer produced, in every dimension, and tracing the tongue contour in this image. In each image, you are seeing only a partial representation of the tongue at a given point in an utterance, but once you stitch all these representations together, you get the full picture! Below you can see the photo on the left which one would fawn over in the processing phase, as it is perfectly clear where the tongue is; then, on the right, you can see another photo, which is more realistic picture of what the processing phase looks like — a computer screen with images of the tongue at a point in time in every dimension with the audio and current reconstruction of the tongue available to help you.</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/210111_ultrasound-tongue.png" alt="" title="example image"> </div> <div class="col-sm-6 mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/210111_ultrasound-processing.jpg" alt="" title="example image"> </div> </div> <div class="caption"> Left image: ideal image of a mid-sagittal section of the tongue, where the black dots trace the surface of the tongue. Right image: the more realistic scenario --- acoustic information on the left, (mid-)sagittal slice in the middle on top, anterior slice in the middle on bottom, (partial) reconstruction of the tongue on the top right, superior slice on the bottom right. </div> <h3 id="upshot">Upshot</h3> <p>Once we processed all this data, we were able to get reconstructions of the tongue, through time, producing the different articulations. I don’t have the videos at the moment, but I will try to get them soon to post here. For now, you can see the picture below!</p> <div class="row justify-content-md-center"> <img class="img-fluid rounded z-depth-1" src="/assets/img/4d1.png" alt="" title="example image"> </div> <div class="caption"> The tongue configuration for /r/ in the production of the words *lore* and *rock* for a speaker from New Hampshire. The yellow dots are the palate, and the lines are contours of the tongue, with red indicating the middle and green the periphery. The images on the left are visualizing the production as if you are looking at the side of someone's face, specifically their left side. The images on the right would be from the perspective of looking down on the person on the left. You can see this interesting bunching, causing a cavity in the body of the tongue! </div> <h2 id="more-information">More information</h2> <p>I helped present this work as a poster at the 2015 Meeting of the Journal of the American Acoustical Society with <a href="https://sphs.indiana.edu/about/faculty/lulich-steven.html" rel="external nofollow noopener" target="_blank">Steven Lulich</a>, <a href="https://linguistics.indiana.edu/about/faculty/berkson-kelly.html" rel="external nofollow noopener" target="_blank">Kelly Berkson</a> and <a href="https://cl.indiana.edu/~kdejong/" rel="external nofollow noopener" target="_blank">Ken de Jong</a>. It was a really fun trip, and people were very excited about the work. After presenting this work, my main task was documenting the protocols for the experiment, and once I did that, it was basically time for me to leave to graduate school, and I haven’t worked on the project since. The poster had a <a href="/assets/pdf/leftpanelASAposter.pdf">left</a>, <a href="/assets/pdf/centerpanelASAposter.pdf">center</a> and <a href="/assets/pdf/rightpanelASAposter.pdf">right</a> panel, so please feel free to check it out! We also hung an iPad with the videos, which people seemed to love. Again, you may need appropriate access, but please do check out their <a href="https://sphs.indiana.edu/research/publications/2019-acquiring-and-visualing.html" rel="external nofollow noopener" target="_blank">paper</a> on this work as well.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Brandon Rhodes. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>